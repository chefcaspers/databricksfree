{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1a8eb28-662e-488c-9c29-8b5933075817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Lakeflow in Databricks\n",
    "\n",
    "In this notebook we'll introduce Lakeflow by setting up Casper's two main Jobs & Pipelines: `dlt_order_items` and `Orders In Progress`\n",
    "\n",
    "The `dlt_order_items` pipeline processes the raw order events produced by Casper's using the medallion architecture:\n",
    "\n",
    "  1. Bronze: Read JSON formed events that arrive on a Volume and write them into a bronze landing table `all_events`\n",
    "  2. Silver: Explore the `order_created` events into 1 item per row (order_items) silver table\n",
    "  3. Gold: Create multiple gold streaming tables for downstream consumption in SQL analysis, Dashboards, Genie spaces etc.\n",
    "\n",
    "The `Orders In Progress` streaming job:\n",
    "\n",
    "  1. Reads from Bronze table of `dlt_order_items` above\n",
    "  2. Aggregates all events per order and keeps them in the stream state\n",
    "  3. Writes out all the events of an order once `delivered` event comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cbf0423-04e7-47c2-a5c7-4b716999a78a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "We need to initialize data and catalogs before we begin creating our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7022a810-f50f-4635-8ecb-02662e211728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import (\n",
    "    setup_catalog_and_volume,\n",
    "    copy_raw_data_to_volume,\n",
    "    drop_gk_demo_catalog,\n",
    "    initialize_dimension_tables\n",
    ")\n",
    "\n",
    "# Drop existing catalog/volume/table if you need to start fresh\n",
    "drop_gk_demo_catalog(spark)\n",
    "\n",
    "## 1. Setup the catalog and volume\n",
    "setup_catalog_and_volume(spark)\n",
    "\n",
    "## 2. Copy the raw data to the volume\n",
    "copy_raw_data_to_volume()\n",
    "\n",
    "## 3. Initialize the static dimension tables\n",
    "initialize_dimension_tables(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43510644-5fd5-463a-b9d1-fe8e7655ea62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### `order_items_dlt` Declarative Pipeline\n",
    "\n",
    "The code for the pipeline is prepared in the `./pipelines/order_items_dlt` directory. To initialize this code as a declarative pipeline, we need to go to `Jobs & Pipelines` in the main navigation bar and click `Create` and then select `ETL Pipeline`\n",
    "\n",
    "![](./images/lakeflow/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93a54bce-54aa-4027-b921-384ab812a5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In the new page you'll need to:\n",
    "\n",
    "1. Name the pipeline in the top left corner (**order_items**)\n",
    "2. Select `gk_demo` catalog and create a new schema `lakeflow` for all the pipeline assets \n",
    "3. Select `Add existing assets` and select the folder `./pipelines/order_items_dlt/` in this repository for both paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bd59a1c-6826-48cb-a33d-d31d34923020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "![](./images/lakeflow/2.png)\n",
    "![](./images/lakeflow/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9edecbfc-35c5-4774-b189-7b30e666df92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Once you add the pipeline code you'll see this page that provides:\n",
    "\n",
    "1. All pipeline assets (code) in the left hand pane\n",
    "2. Tab based editor in the center pane\n",
    "3. Table & Performance results in the bottom pane \n",
    "4. A visual dependency graph in the right hand pane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "813ca3fc-cb84-4b51-9d48-a016562962a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./images/lakeflow/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "731cbd0f-190e-49fd-b6c4-8aa1364b01ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Click `Run Pipeline` to start the pipeline and watch the panes populate with the results\n",
    "\n",
    "![](./images/lakeflow/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d93f66e-8a96-4903-ab26-1e2906f1e7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We've now completed the main declarative pipeline for Casper's. \n",
    "\n",
    "- **Bronze layer** – ingest the raw JSON events exactly as they arrive.\n",
    "- **Silver layer**– clean / normalize: explode the items array inside each order and add useful columns.\n",
    "- **Gold layer** – aggregate the cleaned data into business-ready tables (order-, item-, brand-, and location-level metrics).\n",
    "\n",
    "By default, everything runs in manual trigger mode to save resources, but you can change it to continuous streaming mode if you like.\n",
    "\n",
    "The code for the transformations in each stage is mostly self explantory but here are some notes on each stage:\n",
    "\n",
    "##### 0. Bronze – all_events\n",
    "- Uses Auto Loader (cloudFiles) to watch the /raw_data directory for new JSON files—one file per event.\n",
    "- No transformation—just lands the raw payload in Delta.\n",
    "\n",
    "Result: a streaming DLT table called all_events.\n",
    "\n",
    "##### 1. Silver – dlt_order_items\n",
    "- Filter only order_created events.\n",
    "- Cast the string timestamp field ts to event_ts (true TIMESTAMP).\n",
    "- Parse the body JSON string into a typed struct (body_obj) with your body_schema.\n",
    "- Explode the items array so each line now represents one item within an order.\n",
    "- Compute `extended_price` = price × qty.\n",
    "- Derive `order_day` for partitioning.\n",
    "- Select tidy columns to output.\n",
    "\n",
    "The table is partitioned by `order_day` for fast date-based queries.\n",
    "\n",
    "##### 2-A. Gold – dlt_order_header\n",
    "Aggregates the Silver table to one row per order:\n",
    "\n",
    "- `order_revenue` – sum of all items’ extended_price.\n",
    "- `total_qty` – total units in the order.\n",
    "- `total_items` – count of distinct line items.\n",
    "- `brands_in_order` – unique list of brand_ids bought.\n",
    "\n",
    "Useful for dashboards that care about order-level revenue rather than individual line items.\n",
    "\n",
    "##### 2-B. Gold – dlt_item_sales_day\n",
    "Daily item performance:\n",
    "\n",
    "- Groups by `item_id` (plus its menu/category/brand) and the order_day.\n",
    "- Emits `units_sold` and gross_revenue for each item per calendar day.\n",
    "\n",
    "Partitioned by day to keep each day’s data together.\n",
    "\n",
    "##### 2-C. Gold – dlt_brand_sales_day\n",
    "Brand-level daily roll-up with two streaming-specific features:\n",
    "\n",
    "Watermark (`withWatermark(\"order_ts\", \"3 hours\"))` – tells Spark to discard data arriving more than 3 h late, keeping state manageable.\n",
    "\n",
    "`approx_count_distinct` – HyperLogLog sketch to estimate unique orders per brand without heavy state storage.\n",
    "\n",
    "Outputs: estimated order count, items sold, revenue for each brand on each day.\n",
    "\n",
    "##### 2-D-1. Gold – dlt_location_sales_hourly\n",
    "Hourly KPIs per ghost-kitchen location:\n",
    "\n",
    "- Apply the same 3-hour watermark.\n",
    "- Round each `order_ts` down to the hour with `date_trunc(\"hour\", …)`.\n",
    "- Aggregate by location and that truncated hour timestamp.\n",
    "- Collect approximate order count and revenue.\n",
    "\n",
    "Partitioned by `hour_ts` to speed queries that slice by time.\n",
    "\n",
    "#### Notes:\n",
    "- Layered medallion architecture keeps raw and transformed data separate, simplifying debugging and schema evolution.\n",
    "- Streaming DLT ensures each layer updates automatically as new order files land.\n",
    "- Watermark + HLL choices strike a balance between accuracy and scalability in a perpetual stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "029615db-ad2e-46eb-bc7b-2833834d5603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### `Orders In Progress` streaming job\n",
    "\n",
    "The `Orders In Progress` streaming job is responsible for aggregating events (by `order_id`) as they arrive and writing them out as a single record when an order has completed (the `delivered` event has arrived).\n",
    "\n",
    "The code for this streaming job is defined using a notebook and is located at `./notebooks/Orders In Progress.ipynb`.\n",
    "\n",
    "Starting the `Jobs & Pipelines` tab in the main left navigation bar, click `Create > Job`\n",
    "\n",
    "![](./images/lakeflow/job1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67492aba-784e-4d30-b0ae-b6b3354e9745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Give the job a new title (**Orders In Progress Stream**) and click `Notebook` under `Add your first task`\n",
    "\n",
    "![](./images/lakeflow/job2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b492326-817b-44db-810f-06689b0533de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Give your task a name (`main`) and specify the source of the job as the notebook path as shown in the image. Finally select `Create Task` and `Run Now` to start the first run of this streaming job.\n",
    "\n",
    "**Note:** the streaming job is written using the `AvailableNow` trigger, so it will execute once and stop. This is done to save resources in the Free edition. To process new data (in future examples), you'll need to manually run the job again.\n",
    "\n",
    "![](./images/lakeflow/job3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e47d994a-8c06-4070-abbe-e261fbb2eed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Once the job finishes running you can query the state using the query in the next cell. This leverages [Spark Streaming's State Reader API.](https://docs.databricks.com/aws/en/structured-streaming/read-state)\n",
    "\n",
    "This same query will be used in the `Apps` demo to surface orders in progress to a Databricks App.\n",
    "\n",
    "(**Note** blocked on ES-1538976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5e3191-e46f-459f-92fe-d26cf12583cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "    key.order_id as order_id,\n",
    "    collect_list(list_element) as events \n",
    "FROM read_statestore(\n",
    "    \"/Volumes/gk_demo/default/checkpoints/orders_in_progress\",\n",
    "    stateVarName => 'events' ) \n",
    "GROUP BY key.order_id"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lakeflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
