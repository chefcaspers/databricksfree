{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1a8eb28-662e-488c-9c29-8b5933075817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Lakeflow in Databricks\n",
    "\n",
    "In this notebook we'll set up Casper's main declarative pipeline: `dlt_order_items`. \n",
    "\n",
    "This pipeline will:\n",
    "  1. Read JSON formed events that arrive on a Volume and write them into a bronze landing table `all_events`\n",
    "  2. Normalize `order_created` events into 1 item per row (order_items) silver table\n",
    "  3. Create gold streaming tables for downstream consumption in SQL analysis, Dashboards, Genie spaces etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cbf0423-04e7-47c2-a5c7-4b716999a78a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "We need to initialize data and catalogs before we begin creating our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7022a810-f50f-4635-8ecb-02662e211728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import (\n",
    "    setup_catalog_and_volume,\n",
    "    copy_raw_data_to_volume,\n",
    "    drop_gk_demo_catalog,\n",
    "    initialize_dimension_tables\n",
    ")\n",
    "\n",
    "# Drop existing catalog/volume/table if you need to start fresh\n",
    "drop_gk_demo_catalog(spark)\n",
    "\n",
    "## 1. Setup the catalog and volume\n",
    "setup_catalog_and_volume(spark)\n",
    "\n",
    "## 2. Copy the raw data to the volume\n",
    "copy_raw_data_to_volume()\n",
    "\n",
    "## 3. Initialize the static dimension tables\n",
    "initialize_dimension_tables(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43510644-5fd5-463a-b9d1-fe8e7655ea62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Creating `order_items_dlt` Declarative Pipeline\n",
    "\n",
    "The code for the pipeline is prepared in the `./pipelines/order_items_dlt` directory. To initialize this code as a declarative pipeline, we need to go to `Jobs & Pipelines` in the main navigation bar and click `Create` and then select `ETL Pipeline`\n",
    "\n",
    "![](./images/lakeflow/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93a54bce-54aa-4027-b921-384ab812a5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In the new page you'll need to:\n",
    "\n",
    "1. Name the pipeline in the top left corner (**order_items**)\n",
    "2. Select `gk_demo` catalog and create a new schema `lakeflow` for all the pipeline assets \n",
    "3. Select `Add existing assets` and select the folder `./pipelines/order_items_dlt/` in this repository for both paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bd59a1c-6826-48cb-a33d-d31d34923020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "![](./images/lakeflow/2.png)\n",
    "![](./images/lakeflow/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9edecbfc-35c5-4774-b189-7b30e666df92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Once you add the pipeline code you'll see this page that provides:\n",
    "\n",
    "1. All pipeline assets (code) in the left hand pane\n",
    "2. Tab based editor in the center pane\n",
    "3. Table & Performance results in the bottom pane \n",
    "4. A visual dependency graph in the right hand pane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "813ca3fc-cb84-4b51-9d48-a016562962a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./images/lakeflow/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "731cbd0f-190e-49fd-b6c4-8aa1364b01ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Click `Run Pipeline` to start the pipeline and watch the panes populate with the results\n",
    "\n",
    "![](./images/lakeflow/5.png)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lakeflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
