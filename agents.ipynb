{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5291d21f-61a0-4dc0-9c97-84a0b65e299f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Agents in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ca0344-f88f-4802-b297-17559b1422e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's create an agent, __RefundAgent__ that identifies orders that are eligible for a refund based on the time it took to deliver the order.\n",
    "\n",
    "To do this we'll need to equip our agent with three tools:\n",
    "\n",
    "  1. __Order Details tool__: This tool will take an `order_id` as an input and return all the events associated with this order\n",
    "  2. __Order Delivery Time tool__: This tool will parse the events data and return the actual time it took to deliver the order\n",
    "  3. __Location Timing tool__: This tool will take a `location` and retrieve the 50/75/99 percentile delivery times for that specific location\n",
    "\n",
    "We'll attach these tools to a tool capable LLM like `llama-3-3` in the Playground and test it out directly in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19d29a91-cfae-4443-80ab-96796d945959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "We will work through the following steps:   \n",
    "1. Initialize the catalog and volume and set up the data\n",
    "2. Define tools for the agent to use\n",
    "3. Use the playground to call the tools and test the agent\n",
    "4. Export the agent from the playground to a notebook\n",
    "\n",
    "This repo includes a zipped data file that contains two weeks of sample delivery data. We will start by unzipping the file and loading the data into a Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12a9d69-729b-4fb5-ae38-1ccc63146a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05f1268-54f3-42b6-9638-23863dca7c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import (\n",
    "    setup_catalog_and_volume,\n",
    "    copy_raw_data_to_volume,\n",
    "    initialize_events_table,\n",
    "    drop_gk_demo_catalog,\n",
    "    initialize_order_delivery_times_view,\n",
    ")\n",
    "\n",
    "# Drop existing catalog/volume/table if you need to start fresh\n",
    "# drop_gk_demo_catalog(spark)\n",
    "\n",
    "## 1. Setup the catalog and volume\n",
    "setup_catalog_and_volume(spark)\n",
    "\n",
    "## 2. Copy the raw data to the volume\n",
    "copy_raw_data_to_volume()\n",
    "\n",
    "## 3. Initialize the events table and order timings view\n",
    "initialize_events_table(spark)\n",
    "initialize_order_delivery_times_view(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902aec53-95cc-44f8-b1d4-5681d09e3836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register Tools\n",
    "\n",
    "We will create three [Unity Catalog functions](https://docs.databricks.com/aws/en/generative-ai/agent-framework/create-custom-tool) that will be used as tools for the agent.\n",
    "\n",
    "### Order Details Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacd2463-5a73-4716-9d46-071587e88d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION gk_demo.default.get_order_details(oid STRING)\n",
    "RETURNS TABLE (\n",
    "  body STRING COMMENT 'Body of the event',\n",
    "  event_type STRING COMMENT 'The type of event',\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  ts STRING COMMENT 'The timestamp of the event',\n",
    "  location STRING COMMENT 'the location of the order'\n",
    ")\n",
    "COMMENT 'Returns all events associated with the order id (oid)'\n",
    "RETURN\n",
    "  SELECT body, event_type, order_id, ts, location\n",
    "  FROM gk_demo.default.all_events ae\n",
    "  WHERE ae.order_id = oid;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79789ea9-1981-448d-b468-009f5f202743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Order delivery time tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448c02c8-9ccf-4aa2-b639-8becc5303a5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION gk_demo.default.get_order_delivery_time(oid STRING)\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  creation_time TIMESTAMP COMMENT 'The timestamp of the first event for the order',\n",
    "  delivery_time TIMESTAMP COMMENT 'The timestamp of the last event for the order',\n",
    "  duration_minutes FLOAT COMMENT 'The total duration from the first to the last event in minutes'\n",
    ")\n",
    "COMMENT 'Returns the first event time, last event time, and total duration for a given order id.'\n",
    "RETURN\n",
    "  WITH MinMaxTimestamps AS (\n",
    "    SELECT\n",
    "      MIN(try_to_timestamp(ts)) as first_event_time,\n",
    "      MAX(try_to_timestamp(ts)) as last_event_time\n",
    "    FROM\n",
    "      gk_demo.default.all_events\n",
    "    WHERE\n",
    "      order_id = oid\n",
    "  )\n",
    "  SELECT\n",
    "    oid as order_id,\n",
    "    first_event_time AS creation_time,\n",
    "    last_event_time AS delivery_time,\n",
    "    CAST(\n",
    "      try_divide(\n",
    "        (UNIX_TIMESTAMP(last_event_time) - UNIX_TIMESTAMP(first_event_time)),\n",
    "        60\n",
    "      ) AS FLOAT\n",
    "    ) AS duration_minutes\n",
    "  FROM\n",
    "    MinMaxTimestamps;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f860167-030e-43e6-b5e6-05a9c0ea3ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Order Timings Tool\n",
    "\n",
    "This tool will return the 50/75/99th percentile of total delivery times for a given location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857f67a1-cedf-41f8-94a7-1e821a9202c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION gk_demo.default.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile',\n",
    "  P75 FLOAT COMMENT '75th percentile',\n",
    "  P99 FLOAT COMMENT '99th percentile'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of total delivery times for locations'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM gk_demo.default.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339801b3-6d5b-403a-b60d-8c3a89d4939b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Try out the Agent in the Playground\n",
    "\n",
    "1. Copy this system prompt\n",
    "    ```\n",
    "    You are RefundGPT, a CX agent responsible for refunding food delivery orders.\n",
    "\n",
    "    You can call tools to gather the information you need. Start with an `order_id`.\n",
    "\n",
    "    Instructions:\n",
    "    1. Call `order_details(order_id)` first to get event history and confirm the id is valid and the order was delivered.\n",
    "    2. Figure out the delivery duration by calling `get_order_delivery_time(order_id)`.\n",
    "    3. Extract the location (either directly or from the first event's body).\n",
    "    4. Call `get_location_timings(location)` to get the P50/P75/P99 values.\n",
    "    5. Compare actual delivery time to those percentiles to decide on a fair refund.\n",
    "\n",
    "    Output a single-line JSON with these fields:\n",
    "    - `refund_usd` (float),\n",
    "    - `refund_class` (\"none\" | \"partial\" | \"full\"),\n",
    "    - `reason` (short human explanation)\n",
    "\n",
    "    You must return only the JSON. No extra text or markdown.\n",
    "    ```\n",
    "2. Open Playground from the left sidebar\n",
    "3. Paste the system prompt and select an LLM with tool calling capabilities (`Meta Llama 3.3 70B Instruct`)\n",
    "4. Add the three tools we defined above\n",
    "\n",
    "<img src=\"./images/agents/playground_add_tools.png\" width=\"75%\"/>\n",
    "\n",
    "5. Try an the following order_id: `04e9a339e7fb4435b5a084a60edd927f` as an input to see the agent in action. Below we see the agent called both tools and returned a JSON object with a decision about whether the timing data of this order makes this order eligible for a refund.\n",
    "\n",
    "<img src=\"./images/agents/run_agent_in_notebook.png\" width=\"75%\">\n",
    "\n",
    "6. We can export our prototyped agent using __Create Agent Notebook__. This will create an example notebook guiding you through the process of creating and testing a LangChain-based agent, logging it as an MLflow model, evaluating it with MLflow evaluation, and deploying it on Databricks.\n",
    "\n",
    "<img src=\"./images/agents/create_agent_notebook.png\" width=\"75%\">\n",
    "\n",
    "We will walk through the process of defining, testing, evaluating, and deploying an agent in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3667d88-9310-4f27-8a23-54654d87f362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the Agent in Code\n",
    "\n",
    "We will now define the agent in code using the LangChain framework. Doing so enables us to test and iterate on the agent's behavior with more flexibility than we can achieve in the Playground.\n",
    "\n",
    "The code below defines the LangChain agent using the Unity Catalog functions we created above and the `databricks-meta-llama-3-3-70b-instruct` model from Databricks model serving. It also enables MLflow tracing, which will enable us to observe the agent's behavior in MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7a77b5-c31e-49be-818c-37e039e8e12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ddd95f-628c-4404-ac16-64ef58ed811e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eaffea8-1e19-442d-8b17-89d645bbad54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c8ff0e-4ca5-4066-b313-c85e40fc7281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    VectorSearchRetrieverTool,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "system_prompt = \"\"\"You are RefundGPT, a CX agent responsible for refunding food delivery orders.\n",
    "\n",
    "    You can call tools to gather the information you need. Start with an `order_id`.\n",
    "\n",
    "    Instructions:\n",
    "    1. Call `order_details(order_id)` first to get event history and confirm the id is valid and the order was delivered.\n",
    "    2. Figure out the delivery duration by calling `get_order_delivery_time(order_id)`.\n",
    "    3. Extract the location (either directly or from the first event's body).\n",
    "    4. Call `get_location_timings(location)` to get the P50/P75/P99 values.\n",
    "    5. Compare actual delivery time to those percentiles to decide on a fair refund.\n",
    "\n",
    "    Output a single-line JSON with these fields:\n",
    "    - `refund_usd` (float),\n",
    "    - `refund_class` (\"none\" | \"partial\" | \"full\"),\n",
    "    - `reason` (short human explanation)\n",
    "\n",
    "    You must return only the JSON. No extra text or markdown.\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent, enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://docs.databricks.com/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "tools = []\n",
    "\n",
    "uc_tool_names = [\"gk_demo.default.get_order_details\", \"gk_demo.default.get_location_timings\",\n",
    "                 \"gk_demo.default.get_order_delivery_time\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[Sequence[BaseTool], ToolNode],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a8c7cd-a16f-4bd9-b066-c177a7aa944e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are a few points in the code above that are worth calling out:\n",
    "- We use various methods available via the `databricks_langchain` library to configure out LangChain agent to use Databricks models and UC tools. For more details, see the LangChain docs on [Unity Catalog](https://python.langchain.com/docs/integrations/tools/databricks/) and [Databricks](https://python.langchain.com/docs/integrations/providers/databricks/).\n",
    "- `mlflow.langchain.autolog()` enables [MLflow tracing](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langchain/), which provides end-to-end observability for agent workflows. We will see what this looks like soon.\n",
    "- The code uses the `%%writefile` magic to save the agent's code to a file and includes the line `mlflow.models.set_model(AGENT)`. This is a key step in MLflow's [models from code](https://mlflow.org/docs/latest/ml/model/models-from-code/) approach to model logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9a1ec8d-9dfe-4eec-ac3a-4339f8a8aa39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test the Agent in the Notebook and Review Traces\n",
    "\n",
    "The code below loads the agent from the `agent.py` file we created in the previous section and then tests it in a notebook. We included the line `mlflow.langchain.autolog()` in the agent definition file, so an MLflow trace will appear in the notebook and in the MLflow experiment corresponding to this notebook, which you can find in the Experiments tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdad4c6d-3cf8-4b80-a790-e12babfb9e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa99bb08-29e0-4622-ad9b-20f0d59641f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note:** if the cell below returns a `ModuleNotFoundError: No module named 'agent'`, click `Run` and `Detach & re-attach to compute resource` and then re-try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc12a23-8024-466b-9b56-c446e93ea4c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "project_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "# Add the project directory to the system path\n",
    "sys.path.append(project_directory)\n",
    "\n",
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"My order is so late and I demand a refund. Order ID 04e9a339e7fb4435b5a084a60edd927f\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9c3f93-fe3b-485c-9689-02e6663d9efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "As you can see, MLflow tracing gives a complete, end-to-end view of the the agent's execution. It captures inputs, outputs, intermediate steps such as tool calls, and metadata. Tracing also makes it very easy to identify errors and pinpoint the step at which the error occurred.\n",
    "\n",
    "<img src=\"./images/agents/trace.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153d42d8-c0a3-40f9-aba4-2b1f0d5bb514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Next, we will log our agent as an MLflow model. Model logging keeps our agent development reprodudible and versioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b4b55a-348a-4738-b5d6-8dbb430dca83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME, tools\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool in tools:\n",
    "    resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "input_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I want a refund for order 04e9a339e7fb4435b5a084a60edd927f\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example,\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# set the active model to ensure all traces, metrics, etc., are logged to this model\n",
    "mlflow.set_active_model(model_id = logged_agent_info.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3ac234-bc96-4617-9233-4e40e1671786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Evaluate the Agent with MLflow 3\n",
    "\n",
    "In this section, we will evaluate our agent to make sure it is behaving as intended. Clearly defining the expected behavior is an important part of effective evaluation. In our case, we want to make sure that `RefundAgent` only gives refunds based on delayed orders, not based on any other inputs (e.g. a claim that the order was not delivered), and *does* offer refunds for severely delayed orders.\n",
    "\n",
    "These are fairly basic evaluations. In a real scenario, we would want to evaluate a number of additional dimensions. For example, we would want to ensure that the agent is consistent in the refunds it offers (i.e. that it offer similar refunds for similar delays); that it consistently responds in the expected format so the results can be parsed and used as needed; and that \n",
    "\n",
    "### Generate an Evaluation Dataset\n",
    "\n",
    "To test the above, we will generate some inputs where users demand refunds for reasons unrelated to timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e029394-053f-4d41-b2af-11155b2233d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "refund_queries = [\n",
    "    # late orders; unrelated reasons\n",
    "    \"My order never arrived! I want a refund. Order f0b46db05b7d4f1dbe29e6878842d00b.\",\n",
    "    \"My order was missing hot sauce. Make this right or I am never ordering again! 75d895127eef45e69a43f15b40671926\",\n",
    "    \"Please refund my order, my ice cream was melted f883c5d64f4741c182f0567162fc2b06\",\n",
    "    # on-time orders; unrelated reasons\n",
    "    \"My order never arrived! I want a refund. Order 0737517892f447b387fcccd8cbf11c15.\",\n",
    "    \"My order was missing hot sauce. Make this right or I am never ordering again! b433aa2fd00446eb8212f86bf20358d1\",\n",
    "    \"Please refund my order, my ice cream was melted 48f786e9adbe428e88f97f5aef65a08c\",\n",
    "    # Delayed orders\n",
    "    \"9cffad6a425a4070ac9f6f70ef17761d\",\n",
    "    \"My order was really late! 9c63c4433cd44fa0b48149fba5605019\",\n",
    "]\n",
    "\n",
    "data = []\n",
    "# Expand data with all the examples in refund_queries\n",
    "for query in refund_queries:\n",
    "    data.append(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133904e0-cce3-4344-9bb7-00fc4f1751f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a guidelines-based LLM scorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cd7bc6-65bc-40b8-bb29-bcb6410732f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines\n",
    "import mlflow\n",
    "\n",
    "refund_reason = Guidelines(\n",
    "    name=\"refund_reason\",\n",
    "    guidelines=[\"If a refund is offered, its reason must relate to order timing, not to other issues such as missing components.\"]\n",
    ")\n",
    "\n",
    "\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=data,\n",
    "    scorers=[refund_reason],\n",
    "    predict_fn = lambda messages: AGENT.predict({\"messages\": messages})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69d99dd-9a58-45bb-99d1-de6b3221491b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, in the experiments tab, we can see the overview of this evaluation run:\n",
    "\n",
    "<img src=\"./images/agents/eval_overview.png\" width=\"75%\"/>\n",
    "\n",
    "You'll notice that a couple of the evaluations resulted in failures! We can click on them to see more details. Here's one:\n",
    "\n",
    "<img src=\"./images/agents/eval_result.png\" width=\"75%\"/>\n",
    "\n",
    "The agent returned a refund reason unrelated to the timing of the order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993f037d-2038-477e-900f-8329563378a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Iterate on the Model\n",
    "\n",
    "Let's use the information we obtained from evaluation to improve our agent. We will save a new version of the agent file with an updated system prompt that makes it clear that refunds should only be offered on the basis of late delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d687ee-4089-468f-a16d-08c8b677b7b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent_v2.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    VectorSearchRetrieverTool,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "system_prompt = \"\"\"You are RefundGPT, a CX agent responsible for refunding late food delivery orders.\n",
    "\n",
    "    You can call tools to gather the information you need. Start with an `order_id`.\n",
    "\n",
    "    Instructions:\n",
    "    1. Call `order_details(order_id)` first to get event history and confirm the id is valid and the order was delivered.\n",
    "    2. Figure out the delivery duration by calling `get_order_delivery_time(order_id)`.\n",
    "    3. Extract the location (either directly or from the first event's body).\n",
    "    4. Call `get_location_timings(location)` to get the P50/P75/P99 values.\n",
    "    5. Compare actual delivery time to those percentiles to decide on a fair refund.\n",
    "\n",
    "    Only provide refunds for late orders, and use only the tool call results to determine whether a refund is appropriate.\n",
    "\n",
    "    Output a single-line JSON with these fields:\n",
    "    - `refund_usd` (float),\n",
    "    - `refund_class` (\"none\" | \"partial\" | \"full\"),\n",
    "    - `reason` (short human explanation of whether the order was late and, if late, how late the order was)\n",
    "\n",
    "    You must return only the JSON. No extra text or markdown.\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent, enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://docs.databricks.com/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "tools = []\n",
    "\n",
    "uc_tool_names = [\"gk_demo.default.get_order_details\", \"gk_demo.default.get_location_timings\",\n",
    "                 \"gk_demo.default.get_order_delivery_time\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[Sequence[BaseTool], ToolNode],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa2e0ba-244d-43ed-994b-45a1dda55a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME, tools\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool in tools:\n",
    "    resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "input_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I want a refund for order 04e9a339e7fb4435b5a084a60edd927f\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent_v2\",\n",
    "        python_model=\"agent_v2.py\",\n",
    "        input_example=input_example,\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "mlflow.set_active_model(model_id = logged_agent_info.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b961177-40c9-4fc1-ad2e-283dacba834d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's re-run our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9bd7159-ea72-4b13-9c6e-06624476d4a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent_v2 import AGENT\n",
    "import mlflow\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "\n",
    "mlflow.set_active_model(name=\"agent_v2\")\n",
    "\n",
    "refund_queries = [\n",
    "    # late orders; unrelated reasons\n",
    "    \"My order never arrived! I want a refund. Order f0b46db05b7d4f1dbe29e6878842d00b.\",\n",
    "    \"My order was missing hot sauce. Make this right or I am never ordering again! 75d895127eef45e69a43f15b40671926\",\n",
    "    \"Please refund my order, my ice cream was melted f883c5d64f4741c182f0567162fc2b06\",\n",
    "    # on-time orders; unrelated reasons\n",
    "    \"My order never arrived! I want a refund. Order 0737517892f447b387fcccd8cbf11c15.\",\n",
    "    \"My order was missing hot sauce. Make this right or I am never ordering again! b433aa2fd00446eb8212f86bf20358d1\",\n",
    "    \"Please refund my order, my ice cream was melted 48f786e9adbe428e88f97f5aef65a08c\",\n",
    "    # Delayed orders\n",
    "    \"9cffad6a425a4070ac9f6f70ef17761d\",\n",
    "    \"My order was really late! 9c63c4433cd44fa0b48149fba5605019\",\n",
    "]\n",
    "\n",
    "data = []\n",
    "# Expand data with all the examples in refund_queries\n",
    "for query in refund_queries:\n",
    "    data.append(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "refund_reason = Guidelines(\n",
    "    name=\"refund_reason\",\n",
    "    guidelines=[\"If a refund is offered, its reason must relate to order timing, not to other issues such as missing components.\"]\n",
    ")\n",
    "\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=data,\n",
    "    scorers=[refund_reason],\n",
    "    predict_fn = lambda messages: AGENT.predict({\"messages\": messages})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7ff0979-592b-46ad-9682-a6509d45e4ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Now we can use the same approach as above to review the updated evaluation and iterate further if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce3661db-ffdc-40dc-903c-bc2b6576c1d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d04767f-630d-416d-93f4-1be97022b43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "UC_MODEL_NAME = \"gk_demo.default.refund_agent\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb024c58-bad1-42ad-9c63-529bcb79a563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, scale_to_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44992d28-0899-4076-b47f-55d48b77e185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query the Deployed Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cadf5b90-4f68-480b-9c93-46e102367cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6825514109479430,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "agents",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
