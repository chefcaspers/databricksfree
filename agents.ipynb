{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5291d21f-61a0-4dc0-9c97-84a0b65e299f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Agents in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34ca0344-f88f-4802-b297-17559b1422e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's create an agent, __RefundAgent__ that identifies orders that are eligible for a refund based on the time it took to deliver the order.\n",
    "\n",
    "To do this we'll need to equip our agent with two tools:\n",
    "\n",
    "  1. __Order Details tool__: This tool will take an `order_id` as an input and return all the events associated with this order\n",
    "  2. __Location Timing tool__: This tool will take a `location` and retrieve the 50/75/99 percentile delivery times for that specific location\n",
    "\n",
    "We'll attach these tools to a tool capable LLM like `llama-3-3` in the Playground and test it out directly in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19d29a91-cfae-4443-80ab-96796d945959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "We will work through the following steps:   \n",
    "1. Initialize the catalog and volume and set up the data\n",
    "2. Define tools for the agent to use\n",
    "3. Use the playground to call the tools and test the agent\n",
    "4. Export the agent from the playground to a notebook\n",
    "\n",
    "This repo includes a zipped data file that contains two weeks of sample delivery data. We will start by unzipping the file and loading the data into a Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12a9d69-729b-4fb5-ae38-1ccc63146a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05f1268-54f3-42b6-9638-23863dca7c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import (\n",
    "    setup_catalog_and_volume,\n",
    "    copy_raw_data_to_volume,\n",
    "    initialize_events_table,\n",
    "    drop_gk_demo_catalog,\n",
    "    initialize_order_delivery_times_view,\n",
    ")\n",
    "\n",
    "# Drop existing catalog/volume/table if you need to start fresh\n",
    "# drop_gk_demo_catalog(spark)\n",
    "\n",
    "## 1. Setup the catalog and volume\n",
    "setup_catalog_and_volume(spark)\n",
    "\n",
    "## 2. Copy the raw data to the volume\n",
    "copy_raw_data_to_volume()\n",
    "\n",
    "## 3. Initialize the events table and order timings view\n",
    "initialize_events_table(spark)\n",
    "initialize_order_delivery_times_view(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "902aec53-95cc-44f8-b1d4-5681d09e3836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register Tools\n",
    "\n",
    "We will create two [Unity Catalog functions](https://docs.databricks.com/aws/en/generative-ai/agent-framework/create-custom-tool) that will be used as tools for the agent.\n",
    "\n",
    "### Order Details Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacd2463-5a73-4716-9d46-071587e88d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION gk_demo.default.get_order_details(oid STRING)\n",
    "RETURNS TABLE (\n",
    "  body STRING COMMENT 'Body of the event',\n",
    "  event_type STRING COMMENT 'The type of event',\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  ts STRING COMMENT 'The timestamp of the event',\n",
    "  location STRING COMMENT 'the location of the order'\n",
    ")\n",
    "COMMENT 'Returns all events associated with the order id (oid)'\n",
    "RETURN\n",
    "  SELECT body, event_type, order_id, ts, location\n",
    "  FROM gk_demo.default.all_events ae\n",
    "  WHERE ae.order_id = oid;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f860167-030e-43e6-b5e6-05a9c0ea3ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Order Timings Tool\n",
    "\n",
    "This tool will return the 50/75/99th percentile of total delivery times for a given location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857f67a1-cedf-41f8-94a7-1e821a9202c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION gk_demo.default.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile',\n",
    "  P75 FLOAT COMMENT '75th percentile',\n",
    "  P99 FLOAT COMMENT '99th percentile'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of total delivery times for locations'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM gk_demo.default.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "339801b3-6d5b-403a-b60d-8c3a89d4939b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Try out the Agent in the Playground\n",
    "\n",
    "1. Copy this system prompt\n",
    "    ```\n",
    "    You are RefundGPT, a CX agent responsible for refunding food delivery orders.\n",
    "\n",
    "    You can call tools to gather the information you need. Start with an `order_id`.\n",
    "\n",
    "    Instructions:\n",
    "    1. Call `order_details(order_id)` first to get event history.\n",
    "    2. Figure out the delivery duration by comparing timestamps in the events.\n",
    "    3. Extract the location (either directly or from the first event's body).\n",
    "    4. Call `get_location_timings(location)` to get the P50/P75/P99 values.\n",
    "    5. Compare actual delivery time to those percentiles to decide on a fair refund.\n",
    "\n",
    "    Output a single-line JSON with these fields:\n",
    "    - `refund_usd` (float),\n",
    "    - `refund_class` (\"none\" | \"partial\" | \"full\"),\n",
    "    - `reason` (short human explanation)\n",
    "\n",
    "    You must return only the JSON. No extra text or markdown.\n",
    "    ```\n",
    "2. Open Playground from the left sidebar\n",
    "3. Paste the system prompt and select an LLM with tool calling capabilities (`Meta Llama 3.3 70B Instruct`)\n",
    "\n",
    "<img src=\"./images/agents/playground_add_tools.png\" width=\"75%\"/>\n",
    "\n",
    "4. Try an the following order_id: `04e9a339e7fb4435b5a084a60edd927f` as an input to see the agent in action. Below we see the agent called both tools and returned a JSON object with a decision about whether the timing data of this order makes this order eligible for a refund.\n",
    "\n",
    "<img src=\"./images/agents/run_agent_in_notebook.png\" width=\"75%\">\n",
    "\n",
    "5. We can export our prototyped agent using __Create Agent Notebook__. This will create an example notebook guiding you through the process of creating and testing a LangChain-based agent, logging it as an MLflow model, evaluating it with MLflow evaluation, and deploying it on Databricks.\n",
    "\n",
    "<img src=\"./images/agents/create_agent_notebook.png\" width=\"75%\">\n",
    "\n",
    "We will walk through the process of defining, testing, evaluating, and deploying an agent in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Agent in Code\n",
    "\n",
    "We will now define the agent in code using the LangChain framework. Doing so enables us to test and iterate on the agent's behavior with more flexibility than we can achieve in the Playground.\n",
    "\n",
    "The code below defines the LangChain agent using the Unity Catalog functions we created above and the `databricks-meta-llama-3-3-70b-instruct` model from Databricks model serving. It also enables MLflow tracing, which will enable us to observe the agent's behavior in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    VectorSearchRetrieverTool,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "system_prompt = \"\"\"You are RefundGPT, a CX agent responsible for refunding food delivery orders.\n",
    "\n",
    "    You can call tools to gather the information you need. Start with an `order_id`.\n",
    "\n",
    "    Instructions:\n",
    "    1. Call `order_details(order_id)` first to get event history.\n",
    "    2. Figure out the delivery duration by comparing timestamps in the events.\n",
    "    3. Extract the location (either directly or from the first event's body).\n",
    "    4. Call `get_location_timings(location)` to get the P50/P75/P99 values.\n",
    "    5. Compare actual delivery time to those percentiles to decide on a fair refund.\n",
    "\n",
    "    Output a single-line JSON with these fields:\n",
    "    - `refund_usd` (float),\n",
    "    - `refund_class` (\"none\" | \"partial\" | \"full\"),\n",
    "    - `reason` (short human explanation)\n",
    "\n",
    "    You must return only the JSON. No extra text or markdown.\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent, enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://docs.databricks.com/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "tools = []\n",
    "\n",
    "uc_tool_names = [\"gk_demo.default.get_order_details\", \"gk_demo.default.get_location_timings\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[Sequence[BaseTool], ToolNode],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few points in the code above that are worth calling out:\n",
    "- We use various methods available via the `databricks_langchain` library to configure out LangChain agent to use Databricks models and UC tools. For more details, see the LangChain docs on [Unity Catalog](https://python.langchain.com/docs/integrations/tools/databricks/) and [Databricks](https://python.langchain.com/docs/integrations/providers/databricks/).\n",
    "- `mlflow.langchain.autolog()` enables [MLflow tracing](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langchain/), which provides end-to-end observability for agent workflows. We will see what this looks like soon.\n",
    "- The code uses the `%%writefile` magic to save the agent's code to a file and includes the line `mlflow.models.set_model(AGENT)`. This is a key step in MLflow's [models from code](https://mlflow.org/docs/latest/ml/model/models-from-code/) approach to model logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent in the Notebook and Review Traces\n",
    "\n",
    "The code below loads the agent from the `agent.py` file we created in the previous section and then tests it in a notebook. We included the line `mlflow.langchain.autolog()` in the agent definition file, so an MLflow trace will appear in the notebook and in the MLflow experiment corresponding to this notebook, which you can find in the Experiments tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"My order is so late and I demand a refund. Order ID 04e9a339e7fb4435b5a084a60edd927f\"}]})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4842217807214022,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "agents",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
